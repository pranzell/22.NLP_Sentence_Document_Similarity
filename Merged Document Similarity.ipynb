{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity using different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Importing Various packages ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from time import time\n",
    "import operator\n",
    "import scipy.stats as scipy\n",
    "\n",
    "\n",
    "# --- NLTK PACKAGE ---\n",
    "import nltk\n",
    "# Tokenizers\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# --- GENSIM PACKAGE ---\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, doc2vec, Doc2Vec\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_json('../data/squad_train_doc.json')\n",
    "data_train.rename(columns={'passages': 'documents'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segregating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>questions</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>[[What is the Grotto at Notre Dame?, To whom d...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>[[In what city and state did Beyonce  grow up?...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Montana i/mɒnˈtænə/ is a state in the Western...</td>\n",
       "      <td>[[What is its rank in popularion?, What is the...</td>\n",
       "      <td>Montana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The phrase \"in whole or in part\" has been sub...</td>\n",
       "      <td>[[Which phrase is especially contentious withi...</td>\n",
       "      <td>Genocide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The emergence of resistance of bacteria to an...</td>\n",
       "      <td>[[What is the purpose of antibiotic treatment?...</td>\n",
       "      <td>Antibiotics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  [Architecturally, the school has a Catholic ch...   \n",
       "1  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...   \n",
       "2  [Montana i/mɒnˈtænə/ is a state in the Western...   \n",
       "3  [The phrase \"in whole or in part\" has been sub...   \n",
       "4  [The emergence of resistance of bacteria to an...   \n",
       "\n",
       "                                           questions                     title  \n",
       "0  [[What is the Grotto at Notre Dame?, To whom d...  University_of_Notre_Dame  \n",
       "1  [[In what city and state did Beyonce  grow up?...                   Beyoncé  \n",
       "2  [[What is its rank in popularion?, What is the...                   Montana  \n",
       "3  [[Which phrase is especially contentious withi...                  Genocide  \n",
       "4  [[What is the purpose of antibiotic treatment?...               Antibiotics  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Makes a list of list of all contexts per doc and a list of list of all questions per doc\n",
    "'''\n",
    "def get_compact_dataframe():\n",
    "    context_list = []\n",
    "    question_list = []\n",
    "    for doc in data_train.documents:\n",
    "        context_list.append(get_each_context_each_questionSet(doc)[0])\n",
    "        question_list.append(get_each_context_each_questionSet(doc)[1])\n",
    "    return context_list, question_list\n",
    "\n",
    "\n",
    "''' Accepts one document at a time iteratively\n",
    "    Returns list of context per doc and list of set of questions per doc\n",
    "''' \n",
    "def get_each_context_each_questionSet(document):\n",
    "    each_doc_context_list = [document[i]['context'] for i in range(len(document))]\n",
    "    each_doc_question_list = [document[i]['questions'] for i in range(len(document))]\n",
    "    return  each_doc_context_list, each_doc_question_list\n",
    "\n",
    "context_list, question_list = get_compact_dataframe()[0], get_compact_dataframe()[1]\n",
    "compact_dataframe = pd.DataFrame({'title':data_train.title, 'context':context_list, 'questions': question_list})\n",
    "\n",
    "\n",
    "''' Dataframe contains all 442 documents.\n",
    "    Each row contains:\n",
    "        - 1D List of contexts per doc\n",
    "        - 2D List of list of questions per doc\n",
    "        - Title Name of one doc \n",
    "'''\n",
    "compact_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connected Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contains the list all titles\n",
    "title_list = [data_train.title[i] for i in range(data_train.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contains the list of all contexts per doc\n",
    "list_all_context_per_doc = [' '.join(context_list[i]) for i in range(len(context_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contains the list of list of all questions per doc\n",
    "list_all_questions_per_doc = [sum(question_list[i], []) for i in range(len(question_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contains the list of all questions combined\n",
    "list_all_questions = sum(list_all_questions_per_doc, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "## Pre-processing for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' tokenized_context_and_questions contains the list of word tokenized form of contexts plus questions per each doc\n",
    "    untokenized_context_and_questions contains the list of string format of contexts plus questions per each doc\n",
    "'''\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokenized_context_and_questions, untokenized_context_and_questions = [], []\n",
    "\n",
    "for context, question in zip(list_all_context_per_doc, list_all_questions_per_doc):\n",
    "    context_words = [word for word in word_tokenize(context) if word not in stop_words]\n",
    "    question_words = [word for word in word_tokenize(' '.join(question)) if word not in stop_words]\n",
    "    \n",
    "    tokenized_context_and_questions.append(context_words + question_words)\n",
    "    untokenized_context_and_questions.append(' '.join(context_words + question_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BM25 MODEL\n",
    "''' Accepts a list of tokenized words of context plus questions as its training data.\n",
    "'''\n",
    "BM_25_model = gensim.summarization.bm25.BM25(tokenized_context_and_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TFIDF MODEL\n",
    "''' Accepts a list of tokenized words of context plus questions as its training data.\n",
    "'''\n",
    "dictionary = corpora.Dictionary(tokenized_context_and_questions)\n",
    "dictionary.save('/tmp/squad.dict') \n",
    "raw_corpus = [dictionary.doc2bow(each_doc) for each_doc in tokenized_context_and_questions]\n",
    "corpora.MmCorpus.serialize('/tmp/squad.mm', raw_corpus)\n",
    "corpus = corpora.MmCorpus('/tmp/squad.mm')\n",
    "tfidf = models.TfidfModel(corpus) \n",
    "corpus_tfidf = tfidf[corpus]\n",
    "TFIDF_model = similarities.MatrixSimilarity(corpus_tfidf)\n",
    "TFIDF_model.save('/tmp/squad.TFIDF_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doc2Vec MODEL\n",
    "''' Accepts a list of untokenized string format of context plus questions as its training data.\n",
    "'''\n",
    "size = 200\n",
    "window = 50\n",
    "min_count = 1\n",
    "\n",
    "class DocIterator(object):\n",
    "    ## Initailizes document's list(doc1,doc2...) and its label's list('Book_1','Book_2',...)\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.doc_list = doc_list\n",
    "        self.labels_list = labels_list    \n",
    "    ## Assigns label1 to a list of all words in doc1, label2 to all words in doc2, etc.\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield doc2vec.LabeledSentence(words=doc.split(), tags=[self.labels_list[idx]])\n",
    "\n",
    "docLabels = title_list\n",
    "iter_docs = DocIterator(untokenized_context_and_questions, docLabels)\n",
    "Doc2Vec_model = Doc2Vec(iter_docs, size=size, window=window, min_count=min_count, workers=11, dbow_words = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # WMD\n",
    "# ''' Uses a Google pretrained training model to train WMD's model.\n",
    "# '''\n",
    "# WMD_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing MODEL Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Accepts a question(query) to implement BM 25 Model.\n",
    "    Takes a query and word tokenizes it. \n",
    "          'get_scores' - Calculates the similarity distance between the tokenized-query and the document.\n",
    "    \n",
    "    --> Returns a dataframe with Document name, Score and Rank\n",
    "'''\n",
    "\n",
    "def BM25(query):    \n",
    "    scores = BM_25_model.get_scores(query.split(),1)\n",
    "    BM25_dataframe = pd.DataFrame({'Document':data_train.title, 'Score_BM25':scores}).sort_values(by=['Score_BM25'],ascending=False)\n",
    "    BM25_dataframe['Rank_BM25'] = [i for i in range(1, len(data_train.title)+1)]\n",
    "    return BM25_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Accepts a question(query) to implement TF-IDF Model.\n",
    "    Takes a query and word tokenizes it. \n",
    "    'raw_corpus_query' - The word-tokenized query is compared with the dictionary used to train the document. \n",
    "        'corpus_query' - The word-id and word is converted into a corpus.The corpus is then fed to the TF-IDF model.\n",
    "    'similarity_table' - Stores the TF-IDF weights which are then used to get most similiar documents.\n",
    "               'ranks' - Scipy method which compares the similarity weights and sorts is accordingly.\n",
    "    \n",
    "    --> Returns a dataframe with Document name, Score and Rank\n",
    "'''\n",
    "\n",
    "def TFIDF(query): \n",
    "    query_1 = []\n",
    "    query_1.append(word_tokenize(query))\n",
    "    raw_corpus_query = [dictionary.doc2bow(word) for word in query_1]\n",
    "    corpora.MmCorpus.serialize('/tmp/query3.mm',raw_corpus_query)\n",
    "    corpus_query = corpora.MmCorpus('/tmp/query3.mm')\n",
    "    similarity_table = TFIDF_model[corpus_query]\n",
    "    ranks = scipy.rankdata(similarity_table, method = 'max')\n",
    "    similarity_table = list(np.array(similarity_table).flatten())\n",
    "    TFIDF_dataframe = pd.DataFrame({'Document':data_train.title, 'Score_TFIDF':similarity_table}).sort_values(by=['Score_TFIDF'],ascending=False)\n",
    "    TFIDF_dataframe['Rank_TFIDF'] = [i for i in range(1, len(data_train.title)+1)]\n",
    "    return TFIDF_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Accepts a question(query) to implement Doc2Vec Model.\n",
    "    Takes a query and word tokenizes it. \n",
    "       'avg_sentence' - After that the average of the sentenced words are compared with every document.\n",
    "       'most_similar' - Calculates the similarity distance between the avg of tokenized-sentence with every \n",
    "                        document iteratively.\n",
    "    'list_doc_scores' - Returns the sorted list of comparison with each doc in ascending order.\n",
    "    \n",
    "    --> Returns a dataframe with Document name, Score and Rank(top_n, ascending order sorted)\n",
    "'''\n",
    "\n",
    "def Doc2Vec(query):\n",
    "    similarity_score_matrix , list_doc_names, list_doc_scores, list_doc_ranks, rank = [], [], [], [], 1\n",
    "    avg_sentence = np.zeros((200))\n",
    "    count = 0\n",
    "    for word in word_tokenize(query):\n",
    "        if word in Doc2Vec_model.wv.vocab:\n",
    "            avg_sentence +=  Doc2Vec_model[word]\n",
    "            count+=1\n",
    "    if count != 0:\n",
    "        avg_sentence = avg_sentence / count\n",
    "    similarity_score_matrix.append(Doc2Vec_model.docvecs.most_similar([avg_sentence], topn=len(title_list)))\n",
    "    for each_compared_row in similarity_score_matrix[0]:\n",
    "        list_doc_names.append(each_compared_row[0])\n",
    "        list_doc_scores.append(each_compared_row[1])\n",
    "        list_doc_ranks.append(rank)\n",
    "        rank += 1\n",
    "    query_comparison_dataframe = pd.DataFrame({'Document':list_doc_names, 'Score_Doc2Vec':list_doc_scores, 'Rank_Doc2Vec':list_doc_ranks})\n",
    "    return query_comparison_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Accepts a question(query) to implement Word Mover's Distance Model.\n",
    "    Takes a query and word tokenizes it. \n",
    "                  'sent1' - Stores the word-tokenized question.\n",
    "                  'sent2' - Stores the word-tokenized context plus questions iteratively for all documents.\n",
    "    'similarity_distance' - Calculates the similarity distance between sent1 and sent2.\n",
    "    \n",
    "    --> Returns a dataframe with Document name, Score(Similarity Distance) and Rank(Ascending order sorted)\n",
    "'''\n",
    "\n",
    "def WMD(query):\n",
    "    list_doc_names, list_doc_scores, list_doc_ranks, rank = [], [], [], 1\n",
    "    sent1 = word_tokenize(query)\n",
    "    for index in range(len(data_train.title[0:442])):\n",
    "        sent2 = tokenized_context_and_questions[index]\n",
    "        similarity_distance = WMD_model.wmdistance(sent1, sent2)\n",
    "        list_doc_scores.append(similarity_distance)        \n",
    "    wmd_dataframe = pd.DataFrame({'Document':data_train.title[0:442], 'Score_WMD':list_doc_scores}).sort_values(by=['Score_WMD'],ascending=False)\n",
    "    wmd_dataframe['Rank_WMD'] = [i for i in range(1, len(data_train.title[0:442])+1)]\n",
    "    return wmd_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMBINING DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Document_names_Sorted = title_list.copy()\n",
    "Document_names_Sorted.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining all models and returning a list of dataframes for every question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Takes out one document at a time and stores all questions in that doc in 'all_questions_each_doc'\n",
    "    'one_hot_keys' - Makes a list of one-hot-encoded values of 442 values. \n",
    "                     This list will be used for all the questions in one document. \n",
    "                     Thus for that one document, value will be 1 corresponding to the actual document \n",
    "                     and other values will be assigned 0.\n",
    "                     \n",
    "        'frames'  -  Contains the list of 87,433 dataframes: Each dataframe is one-question vs all documents(442)\n",
    "    \n",
    "    Takes out one question at a time, gets the dataframe from each model having document name, score and rank\n",
    "    concats all dataframes into one and stores it in 'each_question_score_all_docs'.\n",
    "    Adds the list of one-hot-keys to every question with actual document to the dataframe and appends in 'frames'.\n",
    "    \n",
    "    Total Rows    = (1 x 442) x 87,433times  = 87,433x442\n",
    "    Total Columns = 9 \n",
    "'''\n",
    "\n",
    "frames = []\n",
    "doc_number = 0\n",
    "\n",
    "for all_questions_each_doc in list_all_questions_per_doc[0:5]:\n",
    "    \n",
    "    one_hot_keys = []\n",
    "    for each_doc in Document_names_Sorted:\n",
    "        if each_doc == title_list[doc_number]:\n",
    "            one_hot_keys.append(1)\n",
    "        else:\n",
    "            one_hot_keys.append(0)\n",
    "\n",
    "    for each_question in all_questions_each_doc[0:1]:\n",
    "        BM_25_Dataframe = BM25(each_question).sort_values(by=['Document'],ascending=True)\n",
    "        TFDIF_Dataframe = TFIDF(each_question).sort_values(by=['Document'],ascending=True)\n",
    "        Doc2Vec_Dataframe = Doc2Vec(each_question).sort_values(by=['Document'],ascending=True)\n",
    "        \n",
    "        #WMD\n",
    "        #WMD_Dataframe = WMD(each_question).sort_values(by=['Document'],ascending=True)\n",
    "        #each_question_score_all_docs = pd.merge(pd.merge(pd.merge(BM_25_Dataframe, TFDIF_Dataframe), Doc2Vec_Dataframe), WMD_Dataframe)\n",
    "        \n",
    "        each_question_score_all_docs = pd.merge(pd.merge(BM_25_Dataframe, TFDIF_Dataframe), Doc2Vec_Dataframe)\n",
    "        list_each_question = [each_question for i in range(442)] \n",
    "        each_question_score_all_docs['Question'] = list_each_question\n",
    "        each_question_score_all_docs['Actual_Document'] = one_hot_keys\n",
    "        frames.append(each_question_score_all_docs)\n",
    "        \n",
    "    doc_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Dataframe : Combining the list of dataframes and Questions List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Score_BM25</th>\n",
       "      <th>Rank_BM25</th>\n",
       "      <th>Score_TFIDF</th>\n",
       "      <th>Rank_TFIDF</th>\n",
       "      <th>Rank_Doc2Vec</th>\n",
       "      <th>Score_Doc2Vec</th>\n",
       "      <th>Question</th>\n",
       "      <th>Actual_Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008_Sichuan_earthquake</td>\n",
       "      <td>0.623699</td>\n",
       "      <td>23</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>27</td>\n",
       "      <td>381</td>\n",
       "      <td>0.112189</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008_Summer_Olympics_torch_relay</td>\n",
       "      <td>6.652963</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>17</td>\n",
       "      <td>266</td>\n",
       "      <td>0.193726</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51st_state</td>\n",
       "      <td>0.621078</td>\n",
       "      <td>220</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>103</td>\n",
       "      <td>353</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASCII</td>\n",
       "      <td>0.620538</td>\n",
       "      <td>253</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>302</td>\n",
       "      <td>366</td>\n",
       "      <td>0.121287</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A_cappella</td>\n",
       "      <td>0.619212</td>\n",
       "      <td>328</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>355</td>\n",
       "      <td>57</td>\n",
       "      <td>0.365747</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adolescence</td>\n",
       "      <td>0.621875</td>\n",
       "      <td>168</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>246</td>\n",
       "      <td>190</td>\n",
       "      <td>0.247007</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adult_contemporary_music</td>\n",
       "      <td>0.622177</td>\n",
       "      <td>144</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>209</td>\n",
       "      <td>114</td>\n",
       "      <td>0.308748</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Affirmative_action_in_the_United_States</td>\n",
       "      <td>0.621735</td>\n",
       "      <td>180</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>220</td>\n",
       "      <td>78</td>\n",
       "      <td>0.346206</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Age_of_Enlightenment</td>\n",
       "      <td>0.620538</td>\n",
       "      <td>254</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>87</td>\n",
       "      <td>150</td>\n",
       "      <td>0.284539</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aircraft_carrier</td>\n",
       "      <td>0.621940</td>\n",
       "      <td>162</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>200</td>\n",
       "      <td>251</td>\n",
       "      <td>0.203036</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Airport</td>\n",
       "      <td>0.618576</td>\n",
       "      <td>348</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>192</td>\n",
       "      <td>284</td>\n",
       "      <td>0.182557</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>0.618714</td>\n",
       "      <td>344</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>335</td>\n",
       "      <td>87</td>\n",
       "      <td>0.338663</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Alexander_Graham_Bell</td>\n",
       "      <td>0.622950</td>\n",
       "      <td>68</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>257</td>\n",
       "      <td>173</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alfred_North_Whitehead</td>\n",
       "      <td>0.623245</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>342</td>\n",
       "      <td>58</td>\n",
       "      <td>0.365601</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Alloy</td>\n",
       "      <td>0.618280</td>\n",
       "      <td>361</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>375</td>\n",
       "      <td>412</td>\n",
       "      <td>0.070421</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Document  Score_BM25  Rank_BM25  \\\n",
       "0                   2008_Sichuan_earthquake    0.623699         23   \n",
       "1          2008_Summer_Olympics_torch_relay    6.652963         10   \n",
       "2                                51st_state    0.621078        220   \n",
       "3                                     ASCII    0.620538        253   \n",
       "4                                A_cappella    0.619212        328   \n",
       "5                               Adolescence    0.621875        168   \n",
       "6                  Adult_contemporary_music    0.622177        144   \n",
       "7   Affirmative_action_in_the_United_States    0.621735        180   \n",
       "8                      Age_of_Enlightenment    0.620538        254   \n",
       "9                          Aircraft_carrier    0.621940        162   \n",
       "10                                  Airport    0.618576        348   \n",
       "11                                   Alaska    0.618714        344   \n",
       "12                    Alexander_Graham_Bell    0.622950         68   \n",
       "13                   Alfred_North_Whitehead    0.623245         50   \n",
       "14                                    Alloy    0.618280        361   \n",
       "\n",
       "    Score_TFIDF  Rank_TFIDF  Rank_Doc2Vec  Score_Doc2Vec  \\\n",
       "0      0.001628          27           381       0.112189   \n",
       "1      0.002257          17           266       0.193726   \n",
       "2      0.000945         103           353       0.133700   \n",
       "3      0.000560         302           366       0.121287   \n",
       "4      0.000490         355            57       0.365747   \n",
       "5      0.000635         246           190       0.247007   \n",
       "6      0.000693         209           114       0.308748   \n",
       "7      0.000673         220            78       0.346206   \n",
       "8      0.001019          87           150       0.284539   \n",
       "9      0.000712         200           251       0.203036   \n",
       "10     0.000726         192           284       0.182557   \n",
       "11     0.000525         335            87       0.338663   \n",
       "12     0.000620         257           173       0.261543   \n",
       "13     0.000516         342            58       0.365601   \n",
       "14     0.000466         375           412       0.070421   \n",
       "\n",
       "                             Question  Actual_Document  \n",
       "0   What is the Grotto at Notre Dame?                0  \n",
       "1   What is the Grotto at Notre Dame?                0  \n",
       "2   What is the Grotto at Notre Dame?                0  \n",
       "3   What is the Grotto at Notre Dame?                0  \n",
       "4   What is the Grotto at Notre Dame?                0  \n",
       "5   What is the Grotto at Notre Dame?                0  \n",
       "6   What is the Grotto at Notre Dame?                0  \n",
       "7   What is the Grotto at Notre Dame?                0  \n",
       "8   What is the Grotto at Notre Dame?                0  \n",
       "9   What is the Grotto at Notre Dame?                0  \n",
       "10  What is the Grotto at Notre Dame?                0  \n",
       "11  What is the Grotto at Notre Dame?                0  \n",
       "12  What is the Grotto at Notre Dame?                0  \n",
       "13  What is the Grotto at Notre Dame?                0  \n",
       "14  What is the Grotto at Notre Dame?                0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat(frames, keys=list_all_questions[0:1026], ignore_index=True)\n",
    "result.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('Combined_Dataframe', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
